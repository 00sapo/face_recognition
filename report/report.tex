%! TEX program =      xelatex
%! TEX bibliography = biber
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

% \documentclass[twocolumn]{article}
\documentclass{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
% \linespread{1.15} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

% \usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
% \usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\huge} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
% \renewcommand\thesubsection{\Roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\Large\scshape\centering}{\thesection}{1em}{} % Change the look of the section titles
% \titleformat{\subsection}[block]{\large}{\thesubsection}{1em}{} % Change the look of the section titles
% \titleformat{\subsubsection}[block]{\large}{\thesubsubsection}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage[bibstyle=numeric,citestyle=authoryear,backend=biber,natbib=true,maxcitenames=2]{biblatex}


\newrobustcmd*{\parentexttrack}[1]{%
	\begingroup
	\blx@blxinit
	\blx@setsfcodes
	\blx@bibopenparen#1\blx@bibcloseparen
\endgroup}

\AtEveryCite{%
	\let\parentext=\parentexttrack%
	\let\bibopenparen=\bibopenbracket%
\let\bibcloseparen=\bibclosebracket}
\addbibresource{bibliography.bib}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\title{Face recognition through RGB-D images and matrices of SVMs} % Article title
\author{%
	\textsc{Federico Simonetta}\thanks{Ingegneria Informatica LM, matricola 1129912} \\[1ex] % Your name
	\normalsize \href{mailto:simonettaf@dei.unipd.it}{simonettaf@dei.unipd.it} % Your email address
	\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
	\textsc{Alberto Cenzato}\thanks{Informatica LM, matricola xxxxxx} \\[1ex] % Your name
	\normalsize \href{mailto:john@smith.com}{--FILL IN ---} % Your email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent We tried to reimplement the algorithm described in
		\citep{Hayat2016}. We found that their results are reproducible
		with large datasets while with small datasets the SVM models
		have very low precision. We optimized the algorithm for two
		datasets of different size and tried several different
		preprocessing tweaks and decision algorithms. Lastly, we states
		that the biggest contributes to the good training of the SVMs
		are given by the sizes and the noiseness of the dataset.
	\end{abstract}
}


\begin{document}

% Print the title
\Huge
\bfseries
\maketitle
\mdseries
\normalsize

\section{Introduction}

\lettrine[nindent=0em,lines=2]{I}n \citep{Hayat2016} an algorithm for face
recognition based on RGB-D image is given. RGB-D images are a state-of-the-art
scene representation paradigm through wich a 3D scene is described. Compared to
classical RGB images, the RGB-D paradigm adds the depth information that gives
the chance to study images in a three-dimensional space. In their paper, Hayat
et al.\ show that the depth information can really raise precision of face
recognition algorithms.
\\
We tried to reproduce their results using a small dataset consisting of 338
images of 26 different persons. Because of the very poor results, we tried to
train the model with a new dataset of 20 different peoples in 24 sequences for
a total of 15.678 \citep{Fanelli2013}. In \citep{Hayat2016}, authors used the
second of these datasets, merged with other two datasets for a total of more
than 35.000 images.  \\ Our results let us think that this algorithm needs a
very huge dataset for training, and, most of all, it needs a lot of different
people.

\section{Algorithm description}
The algorithm is based on a 2 steps preprocessing and on big matrice of SVMs.

\subsection{Background removal}
\label{sec:background}
First step in preprocessing is the background removal. In \cite{Hayat2016},
they used a simple \textit{k-means} clustering procedure on depth images and
used the nearest cluster to filter out pixels of the RGB images.
\\
In the second dataset we used, this was already done on depth images, so we
have been able to skip this step.
\\
In the first dataset, instead, images were a bit noisy so that, forwarded by
our inital poor results, we tried to optimize this stage.

\subsubsection{Face detection first}
The first optimization was to introduce a face-detection check to remove
segment background only near the face. We used the Haar Cascades classifiers
provided by OpenCV\footnote{For a presentation of the method see \href{https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html}{https://docs.opencv.org/3.3.0/d7/d8b/\\
/tutorial\_py\_face\_detection.html}}
and based on Haar features method proposed by \citep{ViolaJones}.
\\
If no face were detected, a fixed threshold segmentation was performed.
This algorithm allowed to better remove noisy objects near the face of
the person.

\subsubsection{Outlier removal}
We also introduced a step to remove outliers so that \textit{k-means} was
less disturbed. The process was based on OpenCV connected components
algorithms performed on a boolean depth map. Only the connected component
with the maximum area was considered: it was approximated with a rectangle
everything out of the rectangle was discarded.

\subsubsection{Dynamic segmentation with depth histograms}
We also developed an algorithm that was able to dynamically segment depth
images with very good precision traded off against very long computation times.
\\
To evaluate this new algorithm we gave a judgment of the segmentation
quality on all the 338 images of the first dataset. Jugments were given
with boolean values in reference to goodness of the segmentation of the main
object represented in the pictures (in this case a person).
\\
Results gave 92.3\% of precision. Anyway, the longer computational time was not
worth of the precision improvement (only 2\% more than the face-detection-first
algorithm). However, this could be an improvement if very differents images
are used.
\\
The algorithm was based on a histogram computation of depth values and on the
assumption that it was made of "hills", like a lots of gaussians juxtaposed. It
looked for the most frequent value trying to estimate the range containing the
highest "hill". If no face was detected in this range it was re-executed on the
second most high "hill" and so on. Estimation of the range was made analyzing
the logarithm of the second derivative, using fixed thresholds and gradually
enlarging the range. Maybe the most frequent value could be replaced with the
most large range, or the most wide area in a range.

\subsection{Face pose estimation and face cropping}
\label{sec:cropping}
In \citep{Hayat2016}, authors used the algorithm proposed in \citep{Fanelli2011}
to compute a face detection and pose estimation.
\\
We used the same algorithm in the same way. However we choosed to change a bit
the parameters proposed in \citep{Hayat2016} for face cropping.
\\
The basic idea is to look for the first non empty row from the top and for the
first non empty column from the left in the segmented depth image, where
\textit{non empty} means with at least $m$ pixels different from $0$. In the
proposed paper and in our implementations $m = 5$.
\\
The point determined by these coordinates represents the \textit{top-left}
corner of the cropping window.
\\
Next, the window hight is computed enlarging with respect to the distance of
the head from the sensor). Authors proposed height equal to $100/z$, where $z$
is the avarage distance of the head in meters. We instead used $120/z$.
\\
The cropping window width is computed by looking for the first non empty column
starting from the right.
\\
Also, authors made some adaptation to the window \textit{top} edge to better
fit the face position estimated through the algorithm. They also claimed that
their face cropping algorithm, being based on depth values, could make use of a
more precise pose estimation, ending with a more precise face cropping than the
one realized by classical 2D algorithm.
\\
Namely, they recomputed the \textit{top} edge by the following formula:

$$y_t = y_t + (\beta \phi +\gamma \psi)$$

where $\phi$ and $\psi$ are the \textit{roll} and \textit{yaw} angles returned
by the pose estimation of which in \citep{Fanelli2011}, while $\beta$ and
$\gamma$ are parameters setted to $5/8$.
\\
We instead choosed to [FILL IN] ------------------------------------
-------------------------------------------------------------------
\\

\subsection{Image sets representation}
\label{sec:covariances}
Next steps expected to clusterize pose estimation converted from Euler angles to
rotation matrix and to represent each cluster of each person with the covariance
matrix described below.
\\
We followed all of these steps, but we did not convert Euler angles to rotation
matrix; instead, we clusterized just those Euler angles.
\\
\\
After having divided poses informations through \textit{k-means} in $c$
clusters (with $c=3$), we ended up with $2\times c$ sets of images: $c$ sets of RGB
images and $c$ sets of depth images. Next, the algorithm execute these steps
for each set:
\begin{itemize}
	\item divide each cropped image $j$ in $4\times 4$ non overlapping and equally
		spaced distinct blocks
	\item for each block, compute the LBP representation
	\item compute the difference $y(k, j)$ of each $k$-th LBP vector from
		its own mean
	\item compute a $16\times16$ covariance matrix by summing up all products
		between the standard deviations; in formula:

		$$
		C_{p, q} = \frac{1}{n} \sum_{i=1}^n y(p, i)y(q, i)
		$$

		where $i$ is the index of the image, $p$ and $q$ are
		respectively the row and the column index of the covariance
		matrix.
\end{itemize}
Authors of the proposed paper say that this approach to image set representation
is really effective and that, compared to previous usage of covariance based set
representation \citep{Dai2012}, they overcame some issues dued to a $400\times 400$
very huge matrix covariance.

\subsection{SVM training and prediction}
\label{sec:training}
\cite{Hayat2016} is not very clear in the algorithm used to train
the SVMs. We argued the following:
\begin{itemize}
	\item they created a matrix of SVMs with a row for each different person
		and $2\times c$ colums for each person
	\item every covariance matrix is associated to a SVM
	\item every SVM is trained with only one item labeled $1$ and all other
		items labeled $-1$
	\item every SVM is queried with a covariance $16\times 16$ matrix representing
		a query RGB-D image set
\end{itemize}
Authors merged the predictions of all the $2\times n \times c$ SVMs ($n$ is the number of
identities) using the following strategy:
\begin{itemize}
	\item for each of the $2\times c$ SVMs of the person $i$ that classify the
		query set as being the person $i$, person $i$ earn a vote
	\item the identity choosed is the one with the maximum number of votes
	\item if more than one identity has the maximum number of votes, the
		one with the maximum distance from the hyper-plane is choosed
	\item if no SVM classify the query covariance matrix with positive label,
		it is classified as "unknown"
\end{itemize}

We followed the same strategy. To train the SVMs we used the default grid
provided by OpenCV with logstep. We evaluated each combination of parameters of
each SVM with F-measure, so that only one true positive item existed. We used
the mean of the parameters that performed best. We have also verified that the
mean performed good as well with reference to the F-measure.
\\
The proposed paper did not specify any particular training strategy. Because of
this, we tried three different algorithms:
\begin{itemize}
	\item a simple training in which all covariance matrixes but one had
		negative labels, as described above
	\item a training in which all SVMs of the same correct identity had
		positive label
	\item a training (only on the first dataset) in which the covariance
		matrix of the correct identity but of different poses were
		removed from the trainig set
	\item \textit{leave-one-out} --------- ma era sbagliata perché non
		ricreavamo le svm e i risultati di ciascun training non
		venivano usati, riproviamo ----------
\end{itemize}
The strategy to merge results from all the $2 \times n \times c$ SVMs was kept
the same too, but we introduced a rule to achieve a better recognition of
"unknown" faces: if more than $t$ SVMs had the maximum number of votes, the
face was forced to be "unknown". $t$ is a threshold that we setted to $4$.
\\
The type of SVMs tested was the only one proposed by \citep{Hayat2016}, that
was based on the following Stein Kernel:
$$
k(X, Y) = e^{\sigma S(X,Y)}
$$
Where
$$
S(X, Y) = \log\det\left(\frac{X + Y}{2}\right)-\frac{1}{2} \log \det(X\times Y)
$$
This is similar to the RBF kernel function and it accepts the same $\gamma$
parameter. Hence, SVMs were trained looking for the two optimal parameters
$\gamma$ and $C$.

\section{Datasets}
Our initial aim was to adapt the algorithm by \citep{Hayat2016} on a small dataset
containing 338 images of 26 different persons. This dataset contained:
\begin{itemize}
	\item frontal faces at 1 meter from the sensor
	\item frontal faces at 2 meter from the sensor
	\item non frontal faces at 1 meter from the sensor
	\item frontal faces at 1 meter from the sensor with different expressions
	\item different lighting
	\item different backgound scenes
\end{itemize}

The very poor results that we obtained on this dataset forwarded us to
implement all tweaks described in \ref{sec:background}, \ref{sec:cropping},
\ref{sec:training}. Anyway, results did not agree with the ones of the paper.
We can state with a reasonable certainty that the dataset was too small for
training the model.
\\
Therefore, we tested the algorithm on a new dataset, containing almost half of
the images of the original paper \citep{Fanelli2013}, with more than 15.000
images of 20 different persons. On this new dataset, depth images were already
segmented, therefore we did not performed the background removal.

\section{Experiments}
With the first small dataset we made a lot of experiment without succeed.
\\
From the second dataset we randomly extracted 40 images from each person to be
used as testing set and 5 persons were completely removed from the training
set, to simulate the 'unknown' behaviour. Also, we used $1/3$ of the remaining
training set as validation set to evaluate SVMs performance and to find the
best hyper-parameters.
\\
As training strategy we used only the simple training described at the first
point of \ref{sec:training}. Note that maybe a \textit{k-fold cross validation}
could enhance the results.

\section{Conclusions and results}
Our results differ a lot based on which identities were removed from the dataset.
In no one case we removed the persons that have been recorded twice and for whom
there are two sets of images.
\\
All this let us argue that our results still suffer from a too much little
dataset. Probably, a bigger dataset is needed to perform this algorithm. We
also argue that it is not the bigger number of image itself that would made the
algorithm perform better on another dataset, but a larger number of different
identities, because the mean number of images per identity in the dataset from
\citep{Fanelli2013} (about 700 images per identity) is much larger than the
one of the final dataset used by the authors (about 300 images per identity).
\\
Actually, a bigger number of identities means a bigger number of covariance
matrixes and a bigger number of items that we can use to train the SVMs. Also,
this is coherent with Hayat's statement according to which the covariance set
representation is very effective.
\\
In the following table we show our results achieved on multiple executions of
the experiment with different combinations of 'unknown'-'known' identities.
Results are in terms of
\begin{itemize}
	\item \textit{rank-1}: ratio between correct recognitions and the
		total number of sets in the query, where 'unknown'
		identities are considered correct if detected as 'unknown'
	\item \textit{FP-unknown}: ratio between uncorrect recognition of
		'unknown' persons and the total number of 'unknown' persons
\end{itemize}
\begin{table}[]
	\centering
	\caption{Results}
	\label{tab:results}
	\begin{tabular}{|l|l|l|}
		\hline
		\bf Rank-1 & \bf FP-unknown & \bf Comments \\ \hline
		0.89	   & 0.8 &        \\ \hline
		xxxx	   & xxxx & xxxx  \\ \hline
		xxxx	   & xxxx & xxxx  \\ \hline
	\end{tabular}
\end{table}

\section{Technical issues}
During the developement we found a lots of issues dued to a beta stage of
developement in OpenCV. Also, we began using both OpenCV for RGB images and
Point Cloud Library for depth images. Since algorithm \citep{Fanelli2011}
needed depth images represented in OpenCV \textit{Mat} objects, we needed to
switch completely to OpenCV and to leave PCL.
\\
Major issues were found in the use of machine learning module of OpenCV, namely
in kmeans algorithm and in loading and saving from file of custom kernel SVMs.

\printbibliography

\end{document}
